{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSeIWROvFzKw6CGbyA9pwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhangkhare/Learnings/blob/main/Question_Answering_using_Spacy_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy meets Transformers: Fine-tune BERT, XLNet and GPT-2\n",
        "Read blog: https://explosion.ai/blog/spacy-transformers\n",
        "\n",
        "The choice between BERT, GPT-2, and XLNet depends on the specific task and requirements you have. Each model has its strengths and weaknesses, and the \"best\" model will vary depending on the context.\n",
        "\n",
        "1. **BERT (Bidirectional Encoder Representations from Transformers)**:\n",
        "BERT is a pre-trained model developed by Google. It excels in tasks that require a deep understanding of language, such as question answering, text classification, and named entity recognition. BERT's architecture is based on a bidirectional Transformer, which allows it to capture context from both left and right directions. BERT is known for its versatility and has achieved state-of-the-art results in various NLP benchmarks.\n",
        "\n",
        "2. **GPT-2 (Generative Pre-trained Transformer 2):**\n",
        "GPT-2 is a generative language model developed by OpenAI. It is trained to predict the next word in a sentence, making it ideal for tasks like text generation, storytelling, and language translation. GPT-2 has a large number of parameters (1.5 billion), which enables it to generate coherent and contextually relevant text. It has been praised for its creative abilities but may require more data and computational resources compared to BERT for fine-tuning.\n",
        "\n",
        "3. **XLNet (eXtreme Language Understanding Network):**\n",
        "XLNet is another pre-trained model that addresses some limitations of previous models like BERT. It introduces the concept of permutation-based training, allowing the model to consider all possible permutations of the input text, leading to improved context understanding. XLNet performs well on a wide range of tasks, including text classification, natural language inference, and sentiment analysis. It is known for its ability to handle long-range dependencies and has achieved competitive results in various benchmarks.\n",
        "\n",
        "Ultimately, the choice between BERT, GPT-2, and XLNet depends on the specific task, available data, and computational resources. BERT is great for understanding language, GPT-2 is ideal for text generation, and XLNet addresses some limitations of previous models. It is recommended to experiment and evaluate these models on your specific use case to determine which one performs the best for your needs."
      ],
      "metadata": {
        "id": "ejjeiUYLuscJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "FoiECIeuTojq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers\n",
        "!python -m spacy download en_trf_bertbaseuncased_lg # bert-base-uncased\n",
        "#!python -m spacy download de_trf_bertbasecased_lg # bert-base-german-cased\n",
        "#!python -m spacy download en_trf_xlnetbasecased_lg # xlnet-base-cased"
      ],
      "metadata": {
        "id": "z4ahNvYuupwr",
        "outputId": "2777fdc8-54d9-411d-8ae8-691be7bfef77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy-transformers\n",
            "  Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/190.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m184.3/190.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.8/190.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (3.5.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (1.22.4)\n",
            "Collecting transformers<4.31.0,>=3.4.0 (from spacy-transformers)\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.4.6)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers)\n",
            "  Downloading spacy_alignments-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.1.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy-transformers) (2023.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, spacy-alignments, huggingface-hub, transformers, spacy-transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 spacy-alignments-0.9.0 spacy-transformers-1.2.5 tokenizers-0.13.3 transformers-4.30.2\n",
            "2023-06-29 17:49:57.359191: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-29 17:49:58.668548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "\u001b[38;5;1m✘ No compatible package found for 'en_trf_bertbaseuncased_lg' (spaCy\n",
            "v3.5.3)\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Z5bSTDG8TwRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "pjOit7BWU58_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real Data"
      ],
      "metadata": {
        "id": "Lj9X-KI9VRIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('your_file.csv')\n",
        "questions = data['Question'].tolist()\n",
        "answers = data['Answer'].tolist()"
      ],
      "metadata": {
        "id": "6MevE5JQVKW4",
        "outputId": "7c7abeaf-3533-43ff-dbbe-53dfdaad6e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d7b103b8458b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_file.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Data"
      ],
      "metadata": {
        "id": "eqXTn8IfVfLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is the capital of India?\",\n",
        "    \"What is the official language of India?\",\n",
        "    \"Who is the Prime Minister of India?\",\n",
        "    \"What is the currency of India?\",\n",
        "    \"What is the population of India?\",\n",
        "    \"What are the major religions in India?\",\n",
        "    # Add more questions...\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    \"New Delhi\",\n",
        "    \"Hindi\",\n",
        "    \"Narendra Modi\",\n",
        "    \"Indian Rupee\",\n",
        "    \"1.3 billion\",\n",
        "    \"Hinduism, Islam, Christianity, Sikhism, Buddhism, Jainism\",\n",
        "    # Add more answers...\n",
        "]\n",
        "\n",
        "data = pd.DataFrame({\"Question\": questions, \"Answer\": answers})\n",
        "# data.to_csv(\"india_qa.csv\", index=False)\n",
        "questions = data['Question'].tolist()\n",
        "answers = data['Answer'].tolist()"
      ],
      "metadata": {
        "id": "gnVlAc7wUpsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "gP6eXUb0UwWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the data with SpaCy\n",
        "Next, you'll need to process the questions and answers using SpaCy. This involves tokenization, part-of-speech tagging, and dependency parsing. You can use the pre-trained English model en_core_web_sm for this."
      ],
      "metadata": {
        "id": "tijXzl6gVlrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "processed_questions = [nlp(q) for q in questions]\n",
        "processed_answers = [nlp(a) for a in answers]"
      ],
      "metadata": {
        "id": "sZnd2HILVrBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the question answering logic\n",
        "To answer a given question, you'll need to find the most relevant answer from your processed answers. One way to do this is by calculating the similarity between the question and each answer using SpaCy's similarity method. You can then choose the answer with the highest similarity score."
      ],
      "metadata": {
        "id": "rZwtPoabV7dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_similar_answer(question):\n",
        "    question_tokens = nlp(question)\n",
        "    max_similarity = -1\n",
        "    most_similar_answer = None\n",
        "\n",
        "    for answer, processed_answer in zip(answers, processed_answers):\n",
        "        similarity = question_tokens.similarity(processed_answer)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            most_similar_answer = answer\n",
        "\n",
        "    return most_similar_answer"
      ],
      "metadata": {
        "id": "XH1GHtJmV_Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_similar_answer(user_ques):\n",
        "    user_ques_emb = nlp(user_ques)\n",
        "    most_similar_answer = None\n",
        "    count = 0\n",
        "\n",
        "    for q, a in zip(questions, answers):\n",
        "        data_ques_emb = nlp(q)\n",
        "        similarity = user_ques_emb.similarity(data_ques_emb)\n",
        "\n",
        "        if count == 0:\n",
        "            max_similarity = similarity\n",
        "            most_similar_answer = a\n",
        "\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            most_similar_answer = a\n",
        "\n",
        "        count += 1\n",
        "    return most_similar_answer"
      ],
      "metadata": {
        "id": "OsR1TBX2X5Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the question answering system"
      ],
      "metadata": {
        "id": "AzO7YZ2HWEfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Can you tell me India's Capital\"\n",
        "answer = get_most_similar_answer(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "DlGlw9G5WGWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}